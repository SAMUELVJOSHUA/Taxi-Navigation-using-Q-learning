{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AGENT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijw3Ey0r1x7z"
      },
      "outputs": [],
      "source": [
        "# Importing the necessaries\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from q_table import QTable\n",
        "\n",
        "\n",
        "# we define a set of hyperparameters associated with the agent\n",
        "# These influence the potential actions of the agent\n",
        "#In order\n",
        "#Learning rate\n",
        "# Discount factor which determines the rate of arrival of rewards\n",
        "# Probability of selecting action for a state\n",
        "# Rate of reduction of epsilon for better precision in exploration\n",
        "# Minimum value of above mentioned reduction\n",
        "class Agent:\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        obv_sp=500,\n",
        "        acn_sp=6, \n",
        "        aph=0.1,\n",
        "        gma=0.9,\n",
        "        epsn=1.0,\n",
        "        epsn_decay=0.9999,\n",
        "        epsn_min=0.01):\n",
        "      \n",
        "\n",
        "# Initialize the agent with values defined in the hyperparameters  \n",
        "#self is the keyword \n",
        "# note: nA is an instance marking number of actions\n",
        "#instance of Q table belonging to the class Agent in the end \n",
        "        self.nA               = acn_sp\n",
        "        self.possible_actions = np.arange(self.nA)\n",
        "        self.epsn_decay    = epsn_decay\n",
        "        self.epsn          = epsn\n",
        "        self.epsn_min      = epsn_min\n",
        "        \n",
        "        self.q_table          = QTable(\n",
        "            obv_sp=obv_sp,\n",
        "            acn_sp=acn_sp, \n",
        "            aph=aph, \n",
        "            gma=gma\n",
        "        )\n",
        "\n",
        "\n",
        "# Function defined to update epsilon value estimates\n",
        "    def update_epsn(self):\n",
        "        self.epsn = max(self.epsn * self.epsn_decay, self.epsn_min)\n",
        "\n",
        "# Epsilon greedy is defined to \n",
        "# This is to choose the greedy action to get more reward most of the time\n",
        "       \n",
        "        \n",
        "    def epsn_greedy(self, state):\n",
        "        policy                  = np.ones(self.nA) * (self.epsn/self.nA)\n",
        "        best_acn_idx         = np.argmax(self.q_table.q(state))\n",
        "        policy[best_acn_idx] = (1 - self.epsn) + (self.epsn / self.nA)\n",
        "        return policy\n",
        "\n",
        "# We then a select_action function for the agent to be able to chose an action for the specific state\n",
        "# Action is returned as an integer with respect to the corresponding state\n",
        "\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        action_probabilities  = self.epsn_greedy(state)\n",
        "        return np.random.choice(self.possible_actions, p=action_probabilities)\n",
        "\n",
        "#We then update the knowledge of the action to perform next potential step using certain paramters\n",
        "#state denotes the previous state of the environment\n",
        "#action dentoes the agent's previous choice of action\n",
        "#reward denotes the last reward received\n",
        "#next_state denotes the current state of the environment\n",
        "#done denotes whether the episode is complete (True or False)I\n",
        "       \n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "       \n",
        "        self.q_table.sarsa_max_update(state, action, reward, next_state)"
      ]
    }
  ]
}
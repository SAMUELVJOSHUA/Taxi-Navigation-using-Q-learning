{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-TABLE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdv_ZLDM2Ka8"
      },
      "outputs": [],
      "source": [
        "# Importing the necessaries\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#A QTable class is created and the hyperparamerters governing it are defined and updated with values\n",
        "# In order\n",
        "#Learning rate\n",
        "# Discount factor which determines the rate of arrival of rewards\n",
        "# Probability of selecting action for a state\n",
        "# Rate of reduction of epsilon for better precision in exploration\n",
        "# Minimum value of above mentioned reduction\n",
        "\n",
        "class QTable(object):\n",
        "    def __init__(\n",
        "        self, \n",
        "        obv_sp=500,\n",
        "        action_space=6, \n",
        "        alpha=0.5, \n",
        "        gamma=0.9\n",
        "    ):\n",
        "        self.alpha             = alpha\n",
        "        self.gamma             = gamma\n",
        "        self.obv_sp = obv_sp\n",
        "        self.action_space      = action_space\n",
        "        self.__q               = np.zeros(self.obv_sp * self.action_space)\\\n",
        "                                    .reshape((self.obv_sp, self.action_space))\n",
        "\n",
        "# We define our Q learning model\n",
        "#This maps (state, action)\n",
        "    def q(self, state=None, action=None):\n",
        "        if state is None:\n",
        "            return self.__q\n",
        "        if action is None:\n",
        "            return self.__q[state]\n",
        "        return self.__q[state][action]\n",
        "\n",
        "# We then update our Q-learning model, \n",
        "#given a state, an action taken in that state, a new resulting state, and the reward received from taking that action.    \n",
        "    def update_q(self, state, action, value):\n",
        "        self.__q[state][action] = value\n",
        "\n",
        "    def max_q(self, state):\n",
        "        return np.max(self.__q[state])\n",
        "\n",
        "    def old_value(self, state, action):\n",
        "        return (1 - self.alpha) * self.q(state, action)\n",
        "\n",
        "# We define a function to calculate discounted rewards for current time step\n",
        "    def discounted_reward(self, state):\n",
        "        return self.gamma * self.max_q(state)\n",
        "\n",
        "\n",
        "# Now for a new state (new_s)\n",
        "# Estimate the rerward for the new state\n",
        "# Along with all possible actions in the new state\n",
        "# Q table values are thus upated in relation to a new state\n",
        "# Thus new value is updated\n",
        "    def sarsa_max_update(self, s, a, r, new_s):\n",
        "        new_value = self.old_value(s, a) + (self.alpha * (r + self.discounted_reward(new_s) - self.q(s, a)))\n",
        "        self.update_q(s, a, new_value)\n",
        "        \n",
        "    def save(self, score):\n",
        "        timestamp = datetime.datetime.now().timestamp()\n",
        "        timestamp_12_digit = int(timestamp * 1000)\n",
        "        df = pd.DataFrame(self.__q)\n",
        "        df.to_csv(\"alpha_{}_gamma_{}_score_{}__{}.csv\".format(self.alpha, self.gamma, score, timestamp_12_digit))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZAyKM5L32WpP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}